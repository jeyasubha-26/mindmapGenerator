{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPliptTtAnhGwKp/OMZmTLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeyasubha-26/mindmapGenerator/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r7Xl8rRlRD4",
        "outputId": "fdb5e969-45a0-4ce0-ea1d-a75fdd58e4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "model_name = \"google/flan-t5-base\"   # ðŸ”¥ CHANGED\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "class MindmapRequest(BaseModel):\n",
        "    text: str\n",
        "    complexity: int = 3\n",
        "\n",
        "def summarize_if_needed(text, complexity):\n",
        "    if len(text.split()) > 120 and complexity < 7:\n",
        "        prompt = f\"Summarize the following text:\\n{text}\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "        outputs = model.generate(**inputs, max_new_tokens=120)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "def generate_mindmap_logic(text, complexity):\n",
        "    processed_text = summarize_if_needed(text, complexity)\n",
        "\n",
        "    prompt = (\n",
        "        f\"Create a hierarchical mindmap.\\n\"\n",
        "        f\"Complexity: {complexity}/10\\n\"\n",
        "        f\"Text:\\n{processed_text}\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    lines = result.split(\". \")\n",
        "    tree = \"ðŸ§  Mindmap\\n\"\n",
        "    for i, line in enumerate(lines):\n",
        "        tree += \"  \" * (i % (complexity + 1)) + \"â”œâ”€ \" + line + \"\\n\"\n",
        "\n",
        "    return tree\n",
        "\n",
        "@app.post(\"/generate-mindmap\")\n",
        "def generate_mindmap(req: MindmapRequest):\n",
        "    return {\"mindmap\": generate_mindmap_logic(req.text, req.complexity)}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup uvicorn main:app --host 0.0.0.0 --port 8000 &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reM5Cx6Hl-Cc",
        "outputId": "7221c3f4-e190-43a7-9050-a7ea0d27f2a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRpLh8hBnvVL",
        "outputId": "c88fde7d-ad6c-446f-bf83-a9d02be287ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"my authtoken\")\n"
      ],
      "metadata": {
        "id": "_yvwRHmKoHvg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "url = ngrok.connect(8000)\n",
        "print(url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhkP0Z0cnNUv",
        "outputId": "0c8face4-46d7-4388-da49-49d21cc2e241"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"https://1902fd645723.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ]
    }
  ]
}